"""
LLM StateHandler for the AGENTIC platform.
Handles execution of LLM-based states using Anthropic's Claude API.
"""

from typing import Dict, Any, List, Optional
import logging

from app.database import State, StateInput, StateOutput
from app.state_types.base.handler import StateHandler
from app.state_types.base.context import ExecutionContext
from app.integrations.anthropic import get_anthropic_client

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMStateHandler(StateHandler):
    """Handler for LLM states using Anthropic API."""
    
    async def execute(self, state: State, context: ExecutionContext) -> Dict[str, Any]:
        """Execute an LLM state using Anthropic's Claude."""
        logger.info(f"Executing LLM state: {state.name} ({state.id})")
        
        # Get input values from context
        prompt = None
        for input_def in state.inputs:
            if input_def.name == "prompt":
                prompt = context.get_variable(f"{state.id}.{input_def.id}")
                break
        
        if not prompt:
            raise ValueError(f"No prompt found for LLM state {state.id}")
        
        # Get config values
        config = state.config or {}
        model = config.get("model", "claude-3-7-sonnet-20250219")
        temperature = config.get("temperature", 0.7)
        max_tokens = config.get("max_tokens", 1000)
        system_prompt = config.get("system_prompt")
        
        # Apply prompt wrapper if defined
        prompt_prefix = config.get("prompt_prefix", "")
        prompt_suffix = config.get("prompt_suffix", "")
        
        # Construct the final wrapped prompt
        wrapped_prompt = f"{prompt_prefix}{prompt}{prompt_suffix}"
        logger.info(f"Final wrapped prompt for {state.id}: {wrapped_prompt[:100]}...")
        
        # Call Anthropic API
        client = get_anthropic_client()
        response = await client.generate_completion(
            prompt=wrapped_prompt,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system_prompt
        )
        
        # Store results in context
        result = {
            "content": response.content,
            "model": response.model,
            "usage": response.usage
        }
        
        # Set output variables
        outputs = {}
        for output_def in state.outputs:
            if output_def.name == "response":
                variable_name = f"{state.id}.{output_def.id}"
                context.set_variable(variable_name, response.content)
                outputs[output_def.id] = response.content
        
        # Add to history
        context.add_history_entry(
            state_id=state.id,
            action="llm_completion",
            inputs={"prompt": wrapped_prompt},
            outputs=outputs
        )
        
        return outputs
    
    @classmethod
    def get_default_inputs(cls, state_id: str) -> list:
        """Get default input definitions for LLM state."""
        return [
            StateInput(
                id=f"{state_id}-in-0",
                name="prompt",
                data_type="string",
                description="The input prompt to be processed by the LLM"
            )
        ]
    
    @classmethod
    def get_default_outputs(cls, state_id: str) -> list:
        """Get default output definitions for LLM state."""
        return [
            StateOutput(
                id=f"{state_id}-out-0",
                name="response",
                data_type="string",
                description="The response generated by the LLM"
            )
        ]
    
    @classmethod
    def get_default_config(cls) -> Dict[str, Any]:
        """Get default configuration for LLM state."""
        return {
            "model": "claude-3-7-sonnet-20250219",
            "temperature": 0.7,
            "max_tokens": 1000,
            "system_prompt": None,
            "prompt_prefix": "",
            "prompt_suffix": ""
        } 